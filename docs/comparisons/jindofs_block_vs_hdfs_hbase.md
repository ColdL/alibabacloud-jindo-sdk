# HBase on JindoFS 存储模式性能测试

## 一、测试环境
| 主实例组 (MASTER) | 核心实例组 (CORE) |
| --- | --- |
| 主机数量: 3（HA模式）<br />机型规格: ecs.g6a.4xlarge<br />CPU: 16 核 \| 内存: 64G<br />数据盘配置: ESSD云盘 100GB X 1<br />系统盘配置: ESSD云盘 120GB X 1 | 主机数量: 3<br />机型规格: ecs.d2c.6xlarge<br />CPU: 24 核 \| 内存: 88G<br />数据盘配置: 吞吐密集型本地盘 3700GB X 3<br />系统盘配置: ESSD云盘 120GB X 1 |



## 二、测试数据集
采用YCSB工具提供的测试集进行测试 [https://github.com/brianfrankcooper/YCSB](https://github.com/brianfrankcooper/YCSB)
每条记录中10个字段，每个字段大小100B，所以一条记录的数据大小约为1KB左右。每个场景的数据量在场景中表明。


## 三、参数配置
#### 1、HBase配置
```
"hbase.rootdir": "jfs://test/hbase-root"
"hbase.wal.dir": "hdfs://emr-cluster/hbase"
```

#### 2、SmartData配置
  存储模式配置
```
"jfs.namespaces": "test",
"jfs.namespaces.test.mode": "block",
"jfs.namespaces.test.oss.uri": "oss://chexue-bj-load-test/testdata/${clusterId}",
```

  HA配置
```
"jfs.namespace.server.rpc-address": "emr-header-1:8101,emr-header-2:8101,emr-header-3:8101",
"namespace.backend.raft.initial-conf": "emr-header-1:8103:0,emr-header-2:8103:0,emr-header-3:8103:0",
"namespace.backend.type": "raft"
```

#### 3、建表
```
n_splits = 200
create 'usertable', 'family', {SPLITS => (1..n_splits).map {|i| "user#{1000+i*(9999-1000)/n_splits}"}}
```

## 四、测试场景
### 写场景测试
#### 场景1 不同线程数100%写
60w条数据，用不同线程数写入HBase


**JindoFS 存储模式**

|  | 线程数10 | 线程数20 | 线程数30 | 线程数40 |
| --- | --- | --- | --- | --- |
| 总耗时(ms) | 37660 | 20295 | 16584 | 14224 |
| 最小写延迟(us) | 408 | 413 | 410 | 429 |
| 平均写延迟(us) | 607 | 638 | 774 | 876 |
| p95写延迟(us) | 681 | 747 | 946 | 1087 |
| p99写延迟(us) | 807 | 948 | 1382 | 1860 |
| p999写延迟(us) | 3539 | 2963 | 42111 | 31471 |
| 最大写延迟(us) | 52575 | 54399 | 62111 | 82495 |

**HDFS模式**

|  | 线程数10 | 线程数20 | 线程数30 | 线程数40 |
| --- | --- | --- | --- | --- |
| 总耗时(ms) | 38213 | 21500 | 16422 | 15252 |
| 最小写延迟(us) | 413 | 422 | 445 | 432 |
| 平均写延迟(us) | 615 | 676 | 760 | 936 |
| p95写延迟(us) | 696 | 799 | 1027 | 1250 |
| p99写延迟(us) | 1001 | 1240 | 1503 | 2217 |
| p999写延迟(us) | 6327 | 17471 | 14007 | 12999 |
| 最大写延迟(us) | 51583 | 210175 | 63519 | 123071 |

#### 测试分析
从测试数据来看，随着线程数增加，平均单次写入耗时增加，但写入总数据耗时呈下降趋势。


#### 场景2 不同数据量100%写
40个线程，向HBase写入10w，100w和100w条数据


**JindoFS 存储模式**

|  | 10w | 100w | 1000w |
| --- | --- | --- | --- |
| 总耗时(ms) | 3520 | 22936 | 241604 |
| 最小写延迟(us) | 433 | 430 | 413 |
| 平均写延迟(us) | 987 | 872 | 958 |
| p95写延迟(us) | 1366 | 1086 | 1096 |
| p99写延迟(us) | 4299 | 1514 | 1428 |
| p999写延迟(us) | 14879 | 47711 | 30799 |
| 最大写延迟(us) | 77439 | 207487 | 2916351 |

**HDFS模式**

|  | 10w | 100w | 1000w |
| --- | --- | --- | --- |
| 总耗时(ms) | 4154 | 23995 | 246198 |
| 最小写延迟(us) | 473 | 457 | 438 |
| 平均写延迟(us) | 1166 | 909 | 976 |
| p95写延迟(us) | 1432 | 1141 | 1152 |
| p99写延迟(us) | 5423 | 1486 | 1396 |
| p999写延迟(us) | 46655 | 29391 | 32447 |
| 最大写延迟(us) | 231423 | 100223 | 2390015 |

#### 
#### 测试分析
随着数据量增大，HBase在JindoFS 存储模式和HDFS模式下写入性能相近。


### 读场景测试
#### 场景1 不同线程数100%读
60w条数据，用不同线程数从HBase读取数据

**JindoFS 存储模式**

|  | 线程数10 | 线程数20 | 线程数30 | 线程数40 |
| --- | --- | --- | --- | --- |
| 总耗时(ms) | 11171 | 6821 | 6088 | 5801 |
| 最小读延迟(us) | 103 | 105 | 109 | 111 |
| 平均读延迟(us) | 166 | 193 | 253 | 322 |
| p95读延迟(us) | 201 | 270 | 405 | 563 |
| p99读延迟(us) | 293 | 479 | 742 | 930 |
| p999读延迟(us) | 1072 | 3781 | 4919 | 5759 |
| 最大读延迟(us) | 38591 | 34463 | 42879 | 46879 |


**HDFS模式**


|  | 线程数10 | 线程数20 | 线程数30 | 线程数40 |
| --- | --- | --- | --- | --- |
| 总耗时(ms) | 10867 | 6842 | 6072 | 5992 |
| 最小读延迟(us) | 100 | 103 | 108 | 109 |
| 平均读延迟(us) | 160 | 190 | 249 | 324 |
| p95读延迟(us) | 192 | 255 | 413 | 587 |
| p99读延迟(us) | 256 | 390 | 681 | 985 |
| p999读延迟(us) | 308 | 3505 | 4283 | 5479 |
| 最大读延迟(us) | 26495 | 40863 | 40895 | 58367 |

#### 

#### 测试分析
随着线程数增加，读取数据总耗时减少，平均单次读取耗时略微有所增加。由于线程数增加，总体耗时仍然减少。
#### 
#### 场景2 不同数据量100%读
40个线程，从HBase读取10w，100w和100w条数据

**JindoFS 存储模式**


|  | 10w | 100w | 1000w |
| --- | --- | --- | --- |
| 总耗时(ms) | 2075 | 9361 | 75151 |
| 最小读延迟(us) | 112 | 109 | 109 |
| 平均读延迟(us) | 438 | 330 | 294 |
| p95读延迟(us) | 996 | 583 | 502 |
| p99读延迟(us) | 1713 | 934 | 648 |
| p999读延迟(us) | 25455 | 6319 | 2401 |
| 最大读延迟(us) | 50207 | 55583 | 45439 |



**HDFS模式**


|  | 10w | 100w | 1000w |
| --- | --- | --- | --- |
| 总耗时(ms) | 2271 | 8964 | 76636 |
| 最小读延迟(us) | 113 | 111 | 108 |
| 平均读延迟(us) | 471 | 312 | 300 |
| p95读延迟(us) | 970 | 531 | 544 |
| p99读延迟(us) | 2149 | 758 | 682 |
| p999读延迟(us) | 23343 | 5071 | 2159 |
| 最大读延迟(us) | 49311 | 206463 | 51999 |





#### 测试分析
随着测试数据量增加，读取数据性能表现稳定。HBase在JindoFS 存储模式 与HDFS模式这两种模式下读性能相差不大。

#### 场景3 50% Get 和 50% Scan
数据量 60w, 线程数40


**JindoFS 存储模式**



|  | Get | Scan |
| --- | --- | --- |
| 最小读延迟(us) | 72 | 180 |
| 平均读延迟(us) | 4705 | 11960 |
| p95读延迟(us) | 13799 | 24911 |
| p99读延迟(us) | 20911 | 39231 |
| p999读延迟(us) | 216575 | 227199 |
| 最大读延迟(us) | 430079 | 449791 |


**HDFS模式**


|  | Get | Scan |
| --- | --- | --- |
| 最小读延迟(us) | 59 | 181 |
| 平均读延迟(us) | 4116 | 10765 |
| p95读延迟(us) | 12663 | 23119 |
| p99读延迟(us) | 18911 | 32767 |
| p999读延迟(us) | 218623 | 228351 |
| 最大读延迟(us) | 405759 | 414975 |



#### 测试分析
在read和scan混合读场景，HBase在JindoFS 存储模式 与HDFS模式这两种模式下读性能基本相同。

### 读写混合测试
#### 场景1 40%读 60%写
1000w数据集 40个线程 

40%读 + 20% 写入 + 20%更新 + 20%扫描

**JindoFS 存储模式**


|  |  |
| --- | --- |
| 总耗时（ms) | 1224534 |
|  |  |
| 最小读耗时（us) | 119 |
| 平均读耗时（us) | 3160 |
| p95读耗时（us) | 9591 |
| p99读耗时（us) | 16367 |
| 最大读耗时（us) | 1980415 |
|  |  |
| 最小写耗时（us) | 481 |
| 平均写耗时（us) | 3972 |
| p95写耗时（us) | 10455 |
| p99写耗时（us) | 18399 |
| 最大写耗时（us) | 1980415 |
|  |  |
| 最小更新耗时（us) | 439 |
| 平均更新耗时（us) | 3903 |
| p95更新耗时（us) | 10343 |
| p99更新耗时（us) | 18335 |
| 最大更新耗时（us) | 1981439 |
|  |  |
| 最小扫描耗时（us) | 235 |
| 平均扫描耗时（us) | 10031 |
| p95扫描耗时（us) | 19743 |
| p99扫描耗时（us) | 30655 |
| 最大扫描耗时（us) | 1993727 |



**HDFS模式**


|  |  |
| --- | --- |
| 总耗时（ms) | 1472029 |
|  |  |
| 最小读耗时（us) | 150 |
| 平均读耗时（us) | 3868 |
| p95读耗时（us) | 10487 |
| p99读耗时（us) | 55391 |
| 最大读耗时（us) | 3182591 |
|  |  |
| 最小写耗时（us) | 516 |
| 平均写耗时（us) | 3389 |
| p95写耗时（us) | 7811 |
| p99写耗时（us) | 40831 |
| 最大写耗时（us) | 3176447 |
|  |  |
| 最小更新耗时（us) | 439 |
| 平均更新耗时（us) | 3314 |
| p95更新耗时（us) | 7775 |
| p99更新耗时（us) | 40223 |
| 最大更新耗时（us) | 3182591 |
|  |  |
| 最小扫描耗时（us) | 267 |
| 平均扫描耗时（us) | 14863 |
| p95扫描耗时（us) | 47935 |
| p99扫描耗时（us) | 133503 |
| 最大扫描耗时（us) | 3201023 |

#### 
#### 测试分析
HBase在 JindoFS 存储模式下的读写混合的情况下，读/写/更新/扫描等操作的性能表现与HDFS性能表现整体接近。 


#### 场景2 长时间不间断读写测试
同时在三台机器开启40线程对HBase进行长时间的读写混合测试。总共耗时近4天，对HBase进行不间断读写。
三台机器测试的读写量总和为30亿次。每台机器的读写量为10亿次，读写比为4:6。详细比例为：
读取4亿（40%）
写入2亿  (20%)
更新2亿  (20%)
扫描2亿  (20%)

|  | 机器1 | 机器2 | 机器2 |
| --- | --- | --- | --- |
| 总耗时（ms) | 364954714 | 365434660 | 365609622 |
| 最小读耗时（us) | 160 | 117 | 106 |
| 平均读耗时（us) | 82204 | 77687 | 81313 |
| p95读耗时（us) | 402687 | 402687 | 399359 |
| p99读耗时（us) | 829439 | 828927 | 825855 |
| 最大读耗时（us) | 22544383 | 23085055 | 20578303 |
|  |  |  |  |
| 最小写耗时（us) | 512 | 420 | 466 |
| 平均写耗时（us) | 4184 | 4128 | 4168 |
| p95写耗时（us) | 1140 | 1112 | 1188 |
| p99写耗时（us) | 76159 | 76095 | 75711 |
| 最大写耗时（us) | 21643263 | 20398079 | 21118975 |
|  |  |  |  |
| 最小更新耗时（us) | 443 | 469 | 434 |
| 平均更新耗时（us) | 4116 | 4200 | 4126 |
| p95更新耗时（us) | 1082 | 1190 | 1132 |
| p99更新耗时（us) | 75775 | 76351 | 75711 |
| 最大更新耗时（us) | 21643263 | 22085631 | 9322495 |
|  |  |  |  |
| 最小扫描耗时（us) | 323 | 293 | 273 |
| 平均扫描耗时（us) | 553391 | 554668 | 557537 |
| p95扫描耗时（us) | 2252799 | 2256895 | 2258943 |
| p99扫描耗时（us) | 3602431 | 3594239 | 3600383 |
| 最大扫描耗时（us) | 52068351 | 52527103 | 52199423 |



系统性能截图
CPU：

<img src="/pic/jindofs_block_vs_hdfs_hbase_1.png" alt="title" width="800"/>

#### 内存

<img src="/pic/jindofs_block_vs_hdfs_hbase_2.png" alt="title" width="800"/>

网络IO

<img src="/pic/jindofs_block_vs_hdfs_hbase_3.png" alt="title" width="800"/>

磁盘吞吐

<img src="/pic/jindofs_block_vs_hdfs_hbase_4.png" alt="title" width="800"/>

#### 测试分析
三台机器在同时开启40个线程进行总共30亿数据的读写测试情况下，每台机器运行时间接近，同时在4天左右完成整个数据集，且对比三台机器的测试数据，读写性能都表现接近。在4天的运行期间，整个HBase性能表现平稳，无明显qps波动。验证了HBase在JindoFS 存储模式下长时间的运行系统表现平稳。
####
### Compact&Split
#### 场景1
创建测试表，split设置为3。 向表中写入500w数据


**JindoFS 存储模式**



| 总耗时（ms) | 平均写入延迟(us) |
| --- | --- |
| 137741 | 1086 |



**HDFS**



| 总耗时（ms) | 平均写入延迟(us) |
| --- | --- |
| 142040 | 1123 |



观察到触发了split，table的region变成5。

<img src="/pic/jindofs_block_vs_hdfs_hbase_5.png" alt="title" width="800"/>

在写入的时候，有minor compaction触发。

<img src="/pic/jindofs_block_vs_hdfs_hbase_6.png" alt="title" width="800"/>

触发split和compaction的过程中，qps稳定，波动幅度不大，维持在3500左右。

<img src="/pic/jindofs_block_vs_hdfs_hbase_7.png" alt="title" width="800"/>


#### 场景2
对HBase的一张500w数据的表进行读取操作，同时触发Major Compaction


**JindoFS 存储模式**



|  | 触发Major Compaction | 不触发Major Compaction |
| --- | --- | --- |
| 总耗时（ms) | 55556 | 52034 |
| 平均读延迟（us) | 434 | 406 |

观察触发Major Compact期间，系统qps未发生明显波动。总耗时比不触发Major Compact时略有增加，

<img src="/pic/jindofs_block_vs_hdfs_hbase_8.png" alt="title" width="800"/>

**HDFS**



|  | 触发Major Compaction | 不触发Major Compaction |
| --- | --- | --- |
| 总耗时（ms) | 87916 | 54549 |
| 平均读延迟（us) | 690 | 425 |

观察触发Major Compaction期间，系统qps发生波动，qps从84000～99000波动最低下降到4371，Major Compaction结束后qps回升。

<img src="/pic/jindofs_block_vs_hdfs_hbase_9.png" alt="title" width="800"/>

在没有Major Compact的情况下，系统qps维持在80000～99000。

<img src="/pic/jindofs_block_vs_hdfs_hbase_10.png" alt="title" width="800"/>

#### 测试分析
在向HBase写入数据触发Minor Compaction的情况下，HBase写入延迟增加，在两种模式下的写入性能接近。在系统进行Major Compaction的过程中，HBase在JindoFS 存储模式下系统qps下降不明显，在HDFS模式下qps有下降趋势。


通过观察在Major Compaction的worker节点的CPU占有率发现，在HDFS模式下由于Datanode读写存在一把大锁，当major compaction时，由于增加了大量写请求，所以出现CPU增高。而在JindoFS 存储模式下，不会往Datnode中写数据，Compaction过程也只是读写本地和oss文件。所以整体性能JindoFS略优。

HDFS模式下Major Compaction时机器的CPU占有率

<img src="/pic/jindofs_block_vs_hdfs_hbase_11.png" alt="title" width="800"/>

JindoFS 存储模式下Major Compaction时机器的CPU占有率

<img src="/pic/jindofs_block_vs_hdfs_hbase_12.png" alt="title" width="800"/>

### 测试重启
HBase有100w的数据，对系统进行重启。重启了三次，没发现有region长时间处于RIT（Region-In-Transaction)状态。均能正常重启。

<img src="/pic/jindofs_block_vs_hdfs_hbase_13.png" alt="title" width="800"/>

<img src="/pic/jindofs_block_vs_hdfs_hbase_14.png" alt="title" width="800"/>

<img src="/pic/jindofs_block_vs_hdfs_hbase_15.png" alt="title" width="800"/>

### 测试扩容
在EMR集群管理页面对集群进行扩容，增加2台Core实例。HBase和JindoFS的各项服务都能正常启动。RegsionServer数增加为5，且没有长时间处于RIT状态。


<img src="/pic/jindofs_block_vs_hdfs_hbase_16.png" alt="title" width="800"/>

<img src="/pic/jindofs_block_vs_hdfs_hbase_17.png" alt="title" width="800"/>

## 五、运维对比
对于HBase跑在HDFS和 JindoFS 存储模式上，在运维方面主要关注的是存储方面的区别。JindoFS 存储模式与 HDFS 在运维方面相比，大大减轻了存储方面的运维工作，您可以更多地关注在 Hbase 的运维上。通过 JindoFS 存储模式，可以将大部分冷数据存放在对象存储 OSS 上，一方面可以节省成本，另一方面可以借助 OSS 的 SLA 稳定性保证，保证数据安全和服务高可用。更多关于 JindoFS 和 HDFS 的对比，可以[参考链接](./jindofs_block_vs_hdfs.md)。



